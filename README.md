## Visual Question Answering in the Medical Domain

&nbsp;
  The year 2018 witnessed the inauguration of a special challenge for VQA in the medical domain under the name: the VQA-MED challenge [1], which was organized by the reputable ImageCLEF conference [2]. In 2019, the second installment of the VQA-MED challenge [3] is launched and its test dataset currently is available publicly [4].

&nbsp;

### VQA-MED-2018 Challenge:
- [Leaderboard](https://www.crowdai.org/challenges/imageclef-2018-vqa-med/leaderboards)
- Overview of ImageCLEF 2018 Medical Domain Visual Question Answering Task [[Paper](http://ceur-ws.org/Vol-2125/paper_212.pdf)]
- UMass at ImageCLEF Medical Visual Question Answering (Med-VQA) 2018 Task [[Paper](http://ceur-ws.org/Vol-2125/paper_163.pdf)]
- NLM at ImageCLEF 2018 Visual Question Answering in the Medical Domain [[Paper](http://ceur-ws.org/Vol-2125/paper_212.pdf)]
- Employing Inception-Resnet-v2 and BiLSTM for Medical Domain Visual Question Answering [[Paper](http://ceur-ws.org/Vol-2125/paper_107.pdf)][[code](https://github.com/youngzhou97qz/VQA-Med/)]
- JUST at VQA-Med: A VGG-Seq2Seq Model [[Paper](http://ceur-ws.org/Vol-2125/paper_171.pdf)] [[code](https://github.com/bashartalafha/VQA-Med)]
- Deep Neural Networks and Decision Tree classifier for Visual Question Answering in the medical domain [[Paper](http://ceur-ws.org/Vol-2125/paper_159.pdf)]

&nbsp;
### Other Work on 2018 Dataset:
- Visual-Question-Answering-in-Medical-Domain [[Report](https://github.com/nehareddyg/Visual-Question-Answering-in-Medical-Domain/blob/master/AMP%20REPORT.pdf)][[code](https://github.com/nehareddyg/Visual-Question-Answering-in-Medical-Domain)]

&nbsp;
### VQA-MED-2019 Challenge:
- [Leaderboard](https://www.crowdai.org/challenges/imageclef-2019-vqa-med/leaderboards)
- VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019 [[paper](http://ceur-ws.org/Vol-2380/paper_272.pdf)]
- Ensemble of Streamlined Bilinear Visual Question Answering Models for the ImageCLEF 2019 Challenge in the Medical Domain[[paper](http://ceur-ws.org/Vol-2380/paper_64.pdf)]
- Zhejiang University at ImageCLEF 2019 Visual Question Answering in the Medical Domain [[paper](http://ceur-ws.org/Vol-2380/paper_85.pdf)]
- TUA1 at ImageCLEF 2019 VQA-Med: A classification and generation model based on transfer learning [[paper](http://ceur-ws.org/Vol-2380/paper_190.pdf)]
- JUST at ImageCLEF 2019 Visual Question Answering in the Medical Domain [[paper](http://ceur-ws.org/Vol-2380/paper_125.pdf)]
- An Encoder-Decoder model for visual question answering in the medical domain[[paper](http://ceur-ws.org/Vol-2380/paper_124.pdf)]
- Medical Visual Question Answering at Image CLEF 2019- VQA Med [[paper](http://ceur-ws.org/Vol-2380/paper_147.pdf)]
- Tlemcen University at ImageCLEF 2019 Visual Question Answering Task [[paper](http://ceur-ws.org/Vol-2380/paper_117.pdf)]
- Leveraging Medical Visual Question Answering with Supporting Facts [[paper](http://ceur-ws.org/Vol-2380/paper_112.pdf)]
- LSTM in VQA-Med, is it really needed? JCE study on the ImageCLEF 2019 dataset [[paper](http://ceur-ws.org/Vol-2380/paper_116.pdf)] [[code](https://github.com/turner11/VQA-Med)]
- An Xception-GRU Model for Visual Question Answering in the Medical Domain [[paper](http://ceur-ws.org/Vol-2380/paper_127.pdf)]
- Deep Multimodal Learning for Medical Visual Question Answering [[paper](http://ceur-ws.org/Vol-2380/paper_123.pdf)]
- MIT Manipal at ImageCLEF 2019 Visual Question Answering in Medical Domain [[paper](http://ceur-ws.org/Vol-2380/paper_167.pdf)]


&nbsp;
### References
- [1] https://www.imageclef.org/2018/VQA-Med
- [2] https://www.imageclef.org/
- [3] https://www.imageclef.org/2019/medical/vqa
- [4] https://github.com/abachaa/VQA-Med-2019/tree/master/VQAMed2019Test
